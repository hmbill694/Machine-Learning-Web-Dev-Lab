{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"This is the activation function for the network, it will\n",
    "        squishify the results of the network to be between 0 and 1.\n",
    "        It is non-linear  like all other activation functions\"\"\"\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"This is the \"derivative\" of the sigmoid function. \n",
    "        it will be used in the learning phase of the network. \n",
    "        The real derivative of the sigmoid function is\n",
    "        f'(x) = sigmoid(x) x (1- sigmoid(x)). However\n",
    "        when using this in the program my values have already\n",
    "        gone through the sigmoid function so doing so again would be\n",
    "        incorrect\"\"\"\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "\n",
    "def create_weight_matrix_for_layer(num_neurons_in_next_layer,\n",
    "                                   num_neurons_in_current_layer):\n",
    "    \"\"\"This will create a matrix with random values of the shape \n",
    "        R x C where R = the number of neurons in the next layer\n",
    "        and C = the number of neurons in the current layer\"\"\"\n",
    "\n",
    "    return np.random.randn(num_neurons_in_next_layer,num_neurons_in_current_layer)\n",
    "\n",
    "def create_weight_matrix_for_network(network_architecture):\n",
    "    \"\"\"This function will create the weight matrix for an entire network\n",
    "        it takes a list where each member in the list represents the the number\n",
    "        of neurons in that layer. So a input of [5,2,1] represents a network\n",
    "        with 5 neurons in the first layer, 2 in the second and one in the last layer.\n",
    "        It will return a list of n-dimensional numpy arrays where each entry in the list\n",
    "        represents the weights from one layer to the next\"\"\"\n",
    "    \n",
    "    # Okay so what's this slicing about? For the architecture of a network we are able to \n",
    "    # deduce the overall structure of the weight matrix. We will have n-1 weight layers \n",
    "    # per network where n is the total number of network layers. There are no weights for \n",
    "    # incoming data so a network shaped like [5,2,1] will have 2 weight layers. \n",
    "    # The first weight layer will be an array with two rows and 5 columns, the second\n",
    "    # 1 row with 2 columns\n",
    "    column_values = network_architecture[:-1]\n",
    "    row_values = network_architecture[1:]\n",
    "    \n",
    "    return [create_weight_matrix_for_layer(row,col) for row, col in zip(row_values, column_values)]\n",
    "\n",
    "def create_biases_for_layer(num_neurons_in_layer):\n",
    "    \"\"\"Creates a np matrix of (n x 1) for a layer of a network\n",
    "        returns the matrix \"\"\"\n",
    "    return np.random.randn(num_neurons_in_layer,1)\n",
    "\n",
    "def create_bias_matrix_for_network(network_architecture):\n",
    "    \"\"\"The number of bias matrices for a network is n - 1, where\n",
    "        no is the number of layers in the network, including inputs.\n",
    "        Each neuron in a layer will have it's own bias. So a network \n",
    "        of size [5,2,1] will have two biases for one layer and 1 for the next.\n",
    "        The inputs do not get biases.\"\"\"\n",
    "    layers_that_need_biases = network_architecture[1:]\n",
    "    return [create_biases_for_layer(neurons_in_layer) for neurons_in_layer in layers_that_need_biases ]\n",
    "    \n",
    "\n",
    "def back_propagate_error(weight_matrix, actual_values_matrix, guessed_values_matrix):\n",
    "    \"\"\"This function will calculate a error matrix for each hidden layer neuron \"\"\"\n",
    "    actual_values_matrix = np.c_[actual_values_matrix]\n",
    "    guessed_values_matrix = np.c_[guessed_values_matrix]\n",
    "    \n",
    "    # seed error with error between output of network and target output \n",
    "    error = [np.subtract(actual_values_matrix, guessed_values_matrix)]\n",
    "    \n",
    "    # Reverse the weight matrix and then iterate in reverse to\n",
    "    # the last layer, we omit the first layer as that would calculate \n",
    "    # the error for inputs which does not make sense\n",
    "    for index, layer in enumerate(weight_matrix[:0:-1]):\n",
    "        error.append(np.dot(layer.transpose(), error[index]))\n",
    "    return list(reversed(error))\n",
    "\n",
    "def create_deltas(learning_rate,weight_matrix, bias_matrix, error_matrix, activations):\n",
    "    \"\"\"This method will find the changes to each of the weights and biases in the network \n",
    "        it will then return these two lists of deltas\"\"\"\n",
    "    \n",
    "    #lambda function to apply the derivative of sigmoid to the activations array\n",
    "    vectorized_sigmoid_prime = lambda x : sigmoid_prime(x)\n",
    "    \n",
    "    #lists to contain the delta for each layer's weights and biases\n",
    "    delta_weights = []\n",
    "    delta_biases = []\n",
    "    \n",
    "    # the gradients found from applying the sigmoid prime to the activations, omitting the first layer\n",
    "    # which is the inputs, these cannot be altered\n",
    "    gradients = [vectorized_sigmoid_prime(layer) for layer in activations[1:]]\n",
    "    \n",
    "    # iterate over the errors, gradients, weights, biases, and activations\n",
    "    # applying the function delta_wieght_layer = Gradient * lr * Error Layer * activation.T\n",
    "    # the delta_biases = gradients for that layer\n",
    "    for _,error_layer,gradient,activation,bias in zip(weight_matrix,error_matrix, gradients,activations, bias_matrix):\n",
    "        gradient = np.multiply(gradient, learning_rate)\n",
    "        gradient = np.multiply(gradient, error_layer)\n",
    "        delta_layer = np.multiply(activation.transpose(), gradient)\n",
    "        delta_weights.append(delta_layer)\n",
    "        delta_biases.append(gradient)\n",
    "    \n",
    "    #return lists of gathered errors\n",
    "    return delta_weights, delta_biases\n",
    "\n",
    "def train_SGD(learning_rate, input_pairs, weight_matrix, bias_matrix, iterations):\n",
    "    \"\"\"This method will uses stochastic gradient descent to train the network. It will \n",
    "        select a random entry in the input_pairs, which is tuple of lists where [0]: inputs\n",
    "        and where [1]: 1 is the known answer. It will then return the adjusted weights and biases\n",
    "        for the network after executing the training loop for the number of iterations\"\"\"\n",
    "    \n",
    "    # training loop for SGD \n",
    "    for x in range(iterations):\n",
    "        test_item_pair = random.choice(input_pairs)\n",
    "        known_output = test_item_pair[1]\n",
    "        predicted_output, activations = feed_forward(test_item_pair[0],weight_matrix,bias_matrix)\n",
    "        error_matrix = back_propagate_error(weight_matrix,known_output,predicted_output)\n",
    "        delta_weight_matrix, delta_biases_matrix = create_deltas(learning_rate, weight_matrix, \n",
    "                                                                 bias_matrix, error_matrix, activations)\n",
    "        # adjust weights and biases by delta for the layer\n",
    "        weight_matrix = np.add(weight_matrix,delta_weight_matrix)\n",
    "        bias_matrix = np.add(bias_matrix,delta_biases_matrix)\n",
    "    \n",
    "    # adjusted weights and biases for network\n",
    "    return  weight_matrix, bias_matrix\n",
    "        \n",
    "def feed_forward(input_matrix_as_list, network_weight_matrix, network_bias_matrix,guess=False):\n",
    "    \"\"\"This function feeds the inputs into the matrix.\"\"\"\n",
    "    # Store inputs as a column matrix (n x 1)\n",
    "    if isinstance(input_matrix_as_list, (np.ndarray, np.generic) ):\n",
    "        input_matrix_as_list = input_matrix_as_list.tolist()\n",
    "        \n",
    "    output = np.c_[input_matrix_as_list]\n",
    "    layer_activations = [output]\n",
    "    \n",
    "    for bias, weight in zip(network_bias_matrix, network_weight_matrix):\n",
    "        output = sigmoid(np.dot(weight,output ) + bias)\n",
    "        layer_activations.append(output)\n",
    "    \n",
    "    if not guess:\n",
    "        return output, layer_activations\n",
    "    else:\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/Users/harrison/Code_Workspace/Web_Projects/digit-guess/digit-guesser/env/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, arch, inputs, outputs, learning_rate):\n",
    "        self.arch = arch\n",
    "        self.input_pairs = [(x, y) for x, y in zip(inputs, outputs)]\n",
    "        self.weights = NeuralNetwork.create_weights(arch)\n",
    "        self.biases = NeuralNetwork.create_biases(arch)\n",
    "        self.error = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activations = None\n",
    "        self.output = None\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"This is the activation function for the network, it will\n",
    "            squishify the results of the network to be between 0 and 1.\n",
    "            It is non-linear  like all other activation functions\"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_prime(self, x):\n",
    "        \"\"\"This is the \"derivative\" of the sigmoid function.\n",
    "            it will be used in the learning phase of the network.\n",
    "            The real derivative of the sigmoid function is\n",
    "            f'(x) = sigmoid(x) x (1- sigmoid(x)). However\n",
    "            when using this in the program my values have already\n",
    "            gone through the sigmoid function so doing so again would be\n",
    "            incorrect\"\"\"\n",
    "        return x * (1 - x)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_weight_layer(row, col):\n",
    "        \"\"\"This will create a matrix with random values of the shape\n",
    "                R x C where R = the number of neurons in the next layer\n",
    "                and C = the number of neurons in the current layer\"\"\"\n",
    "\n",
    "        return np.random.randn(row, col)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_weights(arch):\n",
    "        \"\"\"This function will create the weight matrix for an entire network\n",
    "                it takes a list where each member in the list represents the the number\n",
    "                of neurons in that layer. So a input of [5,2,1] represents a network\n",
    "                with 5 neurons in the first layer, 2 in the second and one in the last layer.\n",
    "                It will return a list of n-dimensional numpy arrays where each entry in the list\n",
    "                represents the weights from one layer to the next\"\"\"\n",
    "\n",
    "        # Okay so what's this slicing about? For the architecture of a network we are able to\n",
    "        # deduce the overall structure of the weight matrix. We will have n-1 weight layers\n",
    "        # per network where n is the total number of network layers. There are no weights for\n",
    "        # incoming data so a network shaped like [5,2,1] will have 2 weight layers.\n",
    "        # The first weight layer will be an array with two rows and 5 columns, the second\n",
    "        # 1 row with 2 columns\n",
    "        column_values = arch[:-1]\n",
    "        row_values = arch[1:]\n",
    "\n",
    "        return [NeuralNetwork.create_weight_layer(row, col) for row, col in zip(row_values, column_values)]\n",
    "\n",
    "    @staticmethod\n",
    "    def create_bias_layer(layer):\n",
    "        \"\"\"Creates a np matrix of (n x 1) for a layer of a network\n",
    "                returns the matrix \"\"\"\n",
    "        return np.random.randn(layer, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_biases(arch):\n",
    "        \"\"\"The number of bias matrices for a network is n - 1, where\n",
    "                no is the number of layers in the network, including inputs.\n",
    "                Each neuron in a layer will have it's own bias. So a network\n",
    "                of size [5,2,1] will have two biases for one layer and 1 for the next.\n",
    "                The inputs do not get biases.\"\"\"\n",
    "        layers_that_need_biases = arch[1:]\n",
    "        return [NeuralNetwork.create_bias_layer(neurons_in_layer) for neurons_in_layer in layers_that_need_biases]\n",
    "\n",
    "    def feed_forward(self, input_matrix_as_list, guess=False):\n",
    "        \"\"\"This function feeds the inputs into the matrix.\"\"\"\n",
    "        # Store inputs as a column matrix (n x 1)\n",
    "        if isinstance(input_matrix_as_list, (np.ndarray, np.generic)):\n",
    "            input_matrix_as_list = input_matrix_as_list.tolist()\n",
    "\n",
    "        output = np.c_[input_matrix_as_list]\n",
    "        layer_activations = [output]\n",
    "\n",
    "        for bias, weight in zip(self.biases, self.weights):\n",
    "            output = self.sigmoid(np.dot(weight, output) + bias)\n",
    "            layer_activations.append(output)\n",
    "\n",
    "        if not guess:\n",
    "            self.output = output\n",
    "            self.activations = layer_activations\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def back_propagate_error(self, actual_values_matrix, guessed_values_matrix):\n",
    "        \"\"\"This function will calculate a error matrix for each hidden layer neuron \"\"\"\n",
    "        actual_values_matrix = np.c_[actual_values_matrix]\n",
    "        guessed_values_matrix = np.c_[guessed_values_matrix]\n",
    "\n",
    "        # seed error with error between output of network and target output\n",
    "        error = [np.subtract(actual_values_matrix, guessed_values_matrix)]\n",
    "\n",
    "        # Reverse the weight matrix and then iterate in reverse to\n",
    "        # the last layer, we omit the first layer as that would calculate\n",
    "        # the error for inputs which does not make sense\n",
    "        for index, layer in enumerate(self.weights[:0:-1]):\n",
    "            error.append(np.dot(layer.transpose(), error[index]))\n",
    "        self.error = list(reversed(error))\n",
    "\n",
    "    def create_deltas(self):\n",
    "        \"\"\"This method will find the changes to each of the weights and biases in the network\n",
    "            it will then return these two lists of deltas\"\"\"\n",
    "\n",
    "        # lambda function to apply the derivative of sigmoid to the activations array\n",
    "        def vectorized_sigmoid_prime(x): return self.sigmoid_prime(x)\n",
    "\n",
    "        # lists to contain the delta for each layer's weights and biases\n",
    "        delta_weights = []\n",
    "        delta_biases = []\n",
    "\n",
    "        # the gradients found from applying the sigmoid prime to the activations, omitting the first layer\n",
    "        # which is the inputs, these cannot be altered\n",
    "        gradients = [vectorized_sigmoid_prime(layer) for layer in self.activations[1:]]\n",
    "\n",
    "        # iterate over the errors, gradients, weights, biases, and activations\n",
    "        # applying the function delta_weight_layer = Gradient * lr * Error Layer * activation.T\n",
    "        # the delta_biases = gradients for that layer\n",
    "        for _, error_layer, gradient, activation, bias in zip(self.weights, self.error, gradients, self.activations,\n",
    "                                                              self.biases):\n",
    "            gradient = np.multiply(gradient, self.learning_rate)\n",
    "            gradient = np.multiply(gradient, error_layer)\n",
    "            delta_layer = np.multiply(activation.transpose(), gradient)\n",
    "            delta_weights.append(delta_layer)\n",
    "            delta_biases.append(gradient)\n",
    "\n",
    "        # return lists of gathered errors\n",
    "        return delta_weights, delta_biases\n",
    "\n",
    "    def train_SGD(self, iterations):\n",
    "        \"\"\"This method will uses stochastic gradient descent to train the network. It will\n",
    "            select a random entry in the input_pairs, which is tuple of lists where [0]: inputs\n",
    "            and where [1]: 1 is the known answer. It will then return the adjusted weights and biases\n",
    "            for the network after executing the training loop for the number of iterations\"\"\"\n",
    "\n",
    "        # training loop for SGD\n",
    "        for x in range(iterations):\n",
    "            test_item_pair = random.choice(self.input_pairs)\n",
    "            known_output = test_item_pair[1]\n",
    "            self.feed_forward(test_item_pair[0])\n",
    "            self.back_propagate_error(known_output, self.output)\n",
    "            delta_weight_matrix, delta_biases_matrix = self.create_deltas()\n",
    "            # adjust weights and biases by delta for the layer\n",
    "            self.weights = np.add(self.weights, delta_weight_matrix)\n",
    "            self.biases = np.add(self.biases, delta_biases_matrix)\n",
    "\n",
    "    def guess(self, input_list):\n",
    "        \"\"\"Given a sample input list show what the network would output\"\"\"\n",
    "        return self.feed_forward(input_list, True)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_network(network, file_name):\n",
    "        joblib.dump(network,file_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_network(file_name):\n",
    "        return joblib.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "\n",
    "class DataProcessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def normalize_data(self):\n",
    "        pass\n",
    "    \n",
    "    def convert_from_sklearn_to_data_frame(self,in_data_set):\n",
    "        data_set = in_data_set\n",
    "        df = pd.DataFrame(data_set.data,columns=data_set.feature_names)\n",
    "        df[\"target\"] = pd.Series(data_set.target)\n",
    "        return df\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[0.14712704]]\n",
      "[[0.89210058]]\n",
      "[[0.89241023]]\n",
      "[[0.09583739]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "xor_inputs = [[1, 1], [1, 0], [0, 1], [0, 0]]\n",
    "xor_answers = [[0], [1], [1], [0]]\n",
    "\n",
    "\n",
    "arch = [2,2,1]\n",
    "\n",
    "network = NeuralNetwork(arch,inputs=xor_inputs,outputs=xor_answers,learning_rate=.01)\n",
    "\n",
    "network.train_SGD(100000)\n",
    "\n",
    "for input in xor_inputs:\n",
    "    print(network.guess(input))\n",
    "\n",
    "NeuralNetwork.save_network(network, 'network.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[0.14712704]]\n",
      "[[0.89210058]]\n",
      "[[0.89241023]]\n",
      "[[0.09583739]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "network2 = NeuralNetwork.load_network('network.pkl')\n",
    "\n",
    "for input in xor_inputs:\n",
    "    print(network2.guess(input))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}