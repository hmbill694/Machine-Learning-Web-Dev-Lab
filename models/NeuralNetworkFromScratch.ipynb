{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "\n",
    "def convert_from_sklearn_to_data_frame(in_data_set):\n",
    "    data_set = in_data_set\n",
    "    df = pd.DataFrame(data_set.data,columns=data_set.feature_names)\n",
    "    df.target = pd.Series(data_set.target)\n",
    "    return df\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"This is the activation function for the network, it will\n",
    "        squishify the results of the network to be between 0 and 1.\n",
    "        It is non-linear  like all other activation functions\"\"\"\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"This is the derivative of the sigmoid function. \n",
    "        it will be used in the learning phase of the network.\"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def create_weight_matrix_for_layer(num_neurons_in_next_layer,\n",
    "                                   num_neurons_in_current_layer,\n",
    "                            ):\n",
    "    \"\"\"This function will return a weight matrix for a layer in a neural\n",
    "        network. Meaning it returns a numpy array of arrays where there are \n",
    "        the number of neurons in the current layer of weights per\n",
    "        subarray and there are the same number of subarrays as neurons\n",
    "        in the next layers. Also entries will have random values\n",
    "        Ex: If there are 10 neurons in the current layer\n",
    "        and 4 in the next this will return an array of dimensions 4,10 \n",
    "        meaning there are 4 subarrays each with 10 members.\"\"\"\n",
    "\n",
    "    return np.random.randn(num_neurons_in_next_layer,num_neurons_in_current_layer)\n",
    "\n",
    "def create_weight_matrix_for_network(network_architecture):\n",
    "    \"\"\"This function will create the weight matrix for an entire network\n",
    "        it takes a list where each member in the list represents the the number\n",
    "        of neurons in that layer. So a input of [5,2,1] represents a network\n",
    "        with 5 neurons in the first layer, 2 in the second and one in the last layer.\n",
    "        It will return a list of n-dimensional numpy arrays where each entry in the list\n",
    "        represents the weights from one layer to the next\"\"\"\n",
    "    \n",
    "    # Okay so what's this slicing about? For the architecture of a network we are able to \n",
    "    # deduce the overall structure of the weight matrix. We will have n-1 weight layers \n",
    "    # per network where n is the total number of network layers. There are no weights for \n",
    "    # incoming data so a network shaped like [5,2,1] will have 2 weight layers. \n",
    "    # The first weight layer will have 2 arrays each with 5 entries and the second with have 1 array each \n",
    "    num_values_per_sub_array = network_architecture[:-1]\n",
    "    num_of_sub_arrays = network_architecture[1:]\n",
    "    \n",
    "    return [create_weight_matrix_for_layer(y,x) for x, y in zip(num_values_per_sub_array, num_of_sub_arrays)]\n",
    "\n",
    "def create_biases_for_layer(num_neurons_in_layer):\n",
    "    \"\"\"Creates a np matrix of (n x 1) for a layer of a network\n",
    "        returns the matrix \"\"\"\n",
    "    return np.random.randn(num_neurons_in_layer,1)\n",
    "\n",
    "def create_bias_matrix_for_network(network_architecture):\n",
    "    layers_that_need_biases = network_architecture[1:]\n",
    "    return [create_biases_for_layer(neurons_in_layer) for neurons_in_layer in layers_that_need_biases ]\n",
    "    \n",
    "\n",
    "def cost_matrix(weight_matrix,output,predicted_value):\n",
    "    layers_to_propagate_over = practice[:-1]\n",
    "\n",
    "def back_propagation():\n",
    "    pass\n",
    "\n",
    "def train_network():\n",
    "    pass\n",
    "\n",
    "def feed_forward(input_matrix_as_list, network_weight_matrix, network_bias_matrix):\n",
    "    \"\"\"This function feeds the inputs into the matrix.\"\"\"\n",
    "    # Store inputs as a column matrix (n x 1)\n",
    "    input_matrix = np.c_[input_matrix_as_list]\n",
    "    activations = [input_matrix.T]\n",
    "    for bias, weight in zip(network_bias_matrix, network_weight_matrix):\n",
    "        input_matrix = sigmoid(np.dot(weight,input_matrix ) + bias)\n",
    "        activations.append(input_matrix)\n",
    "    return input_matrix, activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Weight matrix  [array([[ 1.04303355,  0.02334845,  0.63326326,  0.2878452 ,  1.95838246,\n",
      "         1.59588919,  0.31257407, -0.14802656,  0.1452024 , -1.86073161],\n",
      "       [-0.32134868,  0.81465846,  2.58171442, -1.42625237, -1.74425894,\n",
      "         1.61546812,  0.10825805, -0.30560267, -0.10669187, -0.80022665]]), array([[ 1.30995025, -0.84511296]])] \n",
      "\n",
      "Bias matrix  [array([[-0.57558694],\n",
      "       [-0.23339531]]), array([[0.07903179]])] \n",
      "\n",
      "output:  [[0.79996476]]\n",
      "Activation Layers:  [array([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]]), array([[0.99868929],\n",
      "       [0.00140925]]), array([[0.79996476]])]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "practice = [10,2,1]\n",
    "\n",
    "\n",
    "biases = create_bias_matrix_for_network(practice)\n",
    "weights = create_weight_matrix_for_network(practice)\n",
    "# \n",
    "print(\"Weight matrix \", weights, \"\\n\")\n",
    "print(\"Bias matrix \", biases, \"\\n\")\n",
    "# \n",
    "# \n",
    "a = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "\n",
    "# \n",
    "# \n",
    "# output_of_network = feed_forward(a, weights, biases)\n",
    "# desired_output = np.ones(output_of_network.shape)\n",
    "# \n",
    "# z = np.subtract(desired_output,output_of_network)**2\n",
    "# \n",
    "# print(z)\n",
    "# \n",
    "# cost = (1.0/practice[-1]) * sum(z)\n",
    "# \n",
    "# \n",
    "# print(\"Output Matrix\", output_of_network)\n",
    "# \n",
    "# print(\"Cost of matrix\", cost)\n",
    "\n",
    "predicted_ouput, activations = feed_forward(a, weights, biases)\n",
    "\n",
    "print(\"output: \", predicted_ouput)\n",
    "print(\"Activation Layers: \", activations)\n",
    "\n",
    "\n",
    "cost = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}